{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Video Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>video_id</th><th>datetime</th><th>title</th><th>transcript</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;bZr2vhoXSy8&quot;</td><td>&quot;2025-02-08T18:10:05Z&quot;</td><td>&quot;I Trained FLUX.1 on My Face (P…</td><td>&quot;flux is a state-of-the-art ima…</td></tr><tr><td>&quot;QvxuR8uLPFs&quot;</td><td>&quot;2025-02-03T18:00:00Z&quot;</td><td>&quot;How to Build Customer Segments…</td><td>&quot;although today&#x27;s AI models are…</td></tr><tr><td>&quot;W4s6b2ZM6kI&quot;</td><td>&quot;2025-01-31T22:38:22Z&quot;</td><td>&quot;Fine-tuning Multimodal Embeddi…</td><td>&quot;multimodal embedding models br…</td></tr><tr><td>&quot;hOLBrIjRAj4&quot;</td><td>&quot;2025-01-22T21:25:16Z&quot;</td><td>&quot;Fine-Tuning Text Embeddings Fo…</td><td>&quot;embedding models represent tex…</td></tr><tr><td>&quot;V1BR2tb_e8g&quot;</td><td>&quot;2025-01-13T21:10:47Z&quot;</td><td>&quot;My AI Development Setup (From …</td><td>&quot;hey everyone I&#x27;m sha I just go…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────┬──────────────────────┬──────────────────────────────┬──────────────────────────────┐\n",
       "│ video_id    ┆ datetime             ┆ title                        ┆ transcript                   │\n",
       "│ ---         ┆ ---                  ┆ ---                          ┆ ---                          │\n",
       "│ str         ┆ str                  ┆ str                          ┆ str                          │\n",
       "╞═════════════╪══════════════════════╪══════════════════════════════╪══════════════════════════════╡\n",
       "│ bZr2vhoXSy8 ┆ 2025-02-08T18:10:05Z ┆ I Trained FLUX.1 on My Face  ┆ flux is a state-of-the-art   │\n",
       "│             ┆                      ┆ (P…                          ┆ ima…                         │\n",
       "│ QvxuR8uLPFs ┆ 2025-02-03T18:00:00Z ┆ How to Build Customer        ┆ although today's AI models   │\n",
       "│             ┆                      ┆ Segments…                    ┆ are…                         │\n",
       "│ W4s6b2ZM6kI ┆ 2025-01-31T22:38:22Z ┆ Fine-tuning Multimodal       ┆ multimodal embedding models  │\n",
       "│             ┆                      ┆ Embeddi…                     ┆ br…                          │\n",
       "│ hOLBrIjRAj4 ┆ 2025-01-22T21:25:16Z ┆ Fine-Tuning Text Embeddings  ┆ embedding models represent   │\n",
       "│             ┆                      ┆ Fo…                          ┆ tex…                         │\n",
       "│ V1BR2tb_e8g ┆ 2025-01-13T21:10:47Z ┆ My AI Development Setup      ┆ hey everyone I'm sha I just  │\n",
       "│             ┆                      ┆ (From …                      ┆ go…                          │\n",
       "└─────────────┴──────────────────────┴──────────────────────────────┴──────────────────────────────┘"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet('data/video.ids.parquet')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (124, 4)\n",
      "n unique rows: 124\n",
      "n unique elements: ( video_id ): 124\n",
      "n unique elements: ( datetime ): 124\n",
      "n unique elements: ( title ): 124\n",
      "n unique elements: ( transcript ): 122\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', df.shape)\n",
    "print('n unique rows:', df.n_unique())\n",
    "for j in range(df.shape[-1]):\n",
    "    print('n unique elements: (', df.columns[j], '):', df[:,j].n_unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of title characters: 6576\n",
      "Total number of transcript characters: 1462250\n"
     ]
    }
   ],
   "source": [
    "print('Total number of title characters:', sum(len(df['title'][i]) for i in range(len(df))))\n",
    "print('Total number of transcript characters:', sum(len(df['transcript'][i]) for i in range(len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>video_id</th><th>datetime</th><th>title</th><th>transcript</th></tr><tr><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;bZr2vhoXSy8&quot;</td><td>2025-02-08 18:10:05</td><td>&quot;I Trained FLUX.1 on My Face (P…</td><td>&quot;flux is a state-of-the-art ima…</td></tr><tr><td>&quot;QvxuR8uLPFs&quot;</td><td>2025-02-03 18:00:00</td><td>&quot;How to Build Customer Segments…</td><td>&quot;although today&#x27;s AI models are…</td></tr><tr><td>&quot;W4s6b2ZM6kI&quot;</td><td>2025-01-31 22:38:22</td><td>&quot;Fine-tuning Multimodal Embeddi…</td><td>&quot;multimodal embedding models br…</td></tr><tr><td>&quot;hOLBrIjRAj4&quot;</td><td>2025-01-22 21:25:16</td><td>&quot;Fine-Tuning Text Embeddings Fo…</td><td>&quot;embedding models represent tex…</td></tr><tr><td>&quot;V1BR2tb_e8g&quot;</td><td>2025-01-13 21:10:47</td><td>&quot;My AI Development Setup (From …</td><td>&quot;hey everyone I&#x27;m sha I just go…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────┬─────────────────────┬───────────────────────────────┬──────────────────────────────┐\n",
       "│ video_id    ┆ datetime            ┆ title                         ┆ transcript                   │\n",
       "│ ---         ┆ ---                 ┆ ---                           ┆ ---                          │\n",
       "│ str         ┆ datetime[μs]        ┆ str                           ┆ str                          │\n",
       "╞═════════════╪═════════════════════╪═══════════════════════════════╪══════════════════════════════╡\n",
       "│ bZr2vhoXSy8 ┆ 2025-02-08 18:10:05 ┆ I Trained FLUX.1 on My Face   ┆ flux is a state-of-the-art   │\n",
       "│             ┆                     ┆ (P…                           ┆ ima…                         │\n",
       "│ QvxuR8uLPFs ┆ 2025-02-03 18:00:00 ┆ How to Build Customer         ┆ although today's AI models   │\n",
       "│             ┆                     ┆ Segments…                     ┆ are…                         │\n",
       "│ W4s6b2ZM6kI ┆ 2025-01-31 22:38:22 ┆ Fine-tuning Multimodal        ┆ multimodal embedding models  │\n",
       "│             ┆                     ┆ Embeddi…                      ┆ br…                          │\n",
       "│ hOLBrIjRAj4 ┆ 2025-01-22 21:25:16 ┆ Fine-Tuning Text Embeddings   ┆ embedding models represent   │\n",
       "│             ┆                     ┆ Fo…                           ┆ tex…                         │\n",
       "│ V1BR2tb_e8g ┆ 2025-01-13 21:10:47 ┆ My AI Development Setup (From ┆ hey everyone I'm sha I just  │\n",
       "│             ┆                     ┆ …                             ┆ go…                          │\n",
       "└─────────────┴─────────────────────┴───────────────────────────────┴──────────────────────────────┘"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change datatime to Datetime dtype\n",
    "df = df.with_columns(pl.col('datetime').cast(pl.Datetime))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  5., 16., 22., 19., 30., 16.,  9.,  1.,  3.]),\n",
       " array([16. , 23.8, 31.6, 39.4, 47.2, 55. , 62.8, 70.6, 78.4, 86.2, 94. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# length characters counts\n",
    "\n",
    "plt.hist(df['title'].str.len_chars())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([45., 15., 21., 12., 17.,  7.,  2.,  2.,  2.,  1.]),\n",
       " array([3.00000e+00, 5.00610e+03, 1.00092e+04, 1.50123e+04, 2.00154e+04,\n",
       "        2.50185e+04, 3.00216e+04, 3.50247e+04, 4.00278e+04, 4.50309e+04,\n",
       "        5.00340e+04]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['transcript'].str.len_chars())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling special strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Text Embeddings For Domain-specific Search (w/ Python)\n",
      "embedding models represent text as semantically meaningful vectors although they can be readily used for countless use cases like retrieval or classification general purpose embedding models may have poor performance on domain specific tasks in this video I'll discuss how we can overcome this limitation via fine tuning I'll start with a highlevel overview of key Concepts and then walk through a concrete example of fine-tuning embeddings on AI J posts okay so let's talk about fine-tuning text embeddings this video is going to assume you already have a basic understanding of what text embeddings do but if you need a refresher I gave a beginner friendly introduction in a previous video so you can check that out if you need to so just jumping right into it Tex embeddings have become more popular these days with the widespread usage of so-called retrieval augmented generation or rag for short and the basic idea behind rag is to improve llm based systems by retrieving relevant content whenever passing a prompt to a large language model a basic example of this is we might have a FAQ bot or a question answering AI assistant where we can pass in a query instead of just passing this query directly to a large language model in a rag system there will be this pre-step of context retrieval and then that query and the context are combined into the prompt which is then fit the large language model to generate a response where embeddings come into this is typically in this context retrieval step the way that works is you'll create this thing called a vector database which has kind of become a buzzword more recently the basic idea of this is taking a set of items in a knowledge base this is typically like chunks of PDF documents or something like that and instead of representing them in their original text in a vector database they are represented by semantic basally meaningful vectors in other words text embedding as a pre-step to building out this retrieval process you'll create this Vector database so at inference time when a query comes in you can simply embed it and then do this so-called similarity search or semantic search where you compute the similarity between the vector representation of the query and all the different items in your knowledge base or your vector database although doing retrieval in this way has become very popular in rag system systems there's one key problem with it and it boils down to this similar doesn't necessarily mean helpful so let me break this down a little bit here in similarity search or also called semantic search we have a vector representation of a query and we have Vector representations of items in a knowledge base we can compute search results by Computing the similarity between this Vector representation of the query and all the items in the knowledge base to make this more concrete the query might be something like how do I update my payment method this might be a very common question so we would expect that it's pretty easy for our system to answer something like this however in practice one thing that might happen is when we do this similarity search the results might be something like this where the top result so in other words the most similar chunk is something like to view your payment history visit the billing section of your account the second most similar result might be payments can be made via credit cards debit cards a and mobile payments it's only the third most similar result that we get something that seems helpful which is to update billing details visit the billing section of your account so this is kind of like the central issue with similarity search which is that just because a query and an item in your knowledge base are similar that doesn't necessarily mean that the item is helpful in answering the query so one thing we can do to overcome these issues is to fine-tune and embedding model for a specific use case so the basic idea of fine-tuning is to adapt a model to a particular use case through additional training so the intuition here is that we might have an original base model which is like a blank slab of marble and the fine tuning process is basically just refining and chiseling this slab into something that is more appropriate for our use case and then of course there's nothing special about a fine-tuned model and it's common practice to fine-tune models multiple times to give it different behaviors and so we we could do additional fine tuning to take a fine-tuned model and make it even more optimized for our use case and so I've actually made a handful of videos on fine-tuning if this is a new idea to you I would recommend checking those out but specifically when it comes to text embeddings the central motivation for fine-tuning is that this idea of similarity varies by use case for example consider these two pieces of text we have one that says what is Rag and another one that says what is fine tun so let's say we were trying to train a comment classifier to basically take YouTube comments and cluster similar ones together and ensure that different ones aren't clustered together in that case these two questions should be represented in similar Ways by our embedding model so that when we do the clustering they will be put into the same group however if we were doing something like an FAQ search the questions what is Rag and what is fine tuning these should be dissimilar things because if we have an incoming query that says what is rag an item in the knowledge base that contains the text what is fine-tuning wouldn't be helpful in answering this query so ideally these should be dissimilar and taking this one step further if we were talking about a different context so instead of doing FAQ search in the context of AI let's say we're doing document search in the context of project management where rag instead of standing for retrieval augmented generation it instead stands for red Amber and green which is like this stoplight system used sometimes in project management then these items should again be dissimilar because this question isn't helpful in answering this query but the representations should be different because the meaning of rag in the context of project management is much different than its meaning in AI so here I'll break down the embedding fine-tuning process into five basic steps the first step is to get rather positive and negative pairs the second step is to pick a pre-trained model and we'll see how we can do this the third step is to pick a loss function which will basically guide the fine-tuning process and then this will depend on the task that you're trying to do and the data that you have available the fourth step is you're going to fine-tune the pre-trained model using your data set and then finally we're going to evaluate the model so instead of talking about these five steps in an abstract way I think it would be helpful to walk through a concrete example so we can see what each of these steps looks like in code so here I'm going to fine-tune a set of text embeddings on AI job listings the basic idea is here we want to take humanlike queries like data scientist 6 years of experience llms credit risk content marketing and we want to match this query with some kind of job post there are two things here that make fine-tuning helpful for this task one is that the the queries and the job descriptions look very different queries are typically very short while job descriptions can be quite long there's a need right there for some kind of fine tuning and additionally there's a lot of technical jargon in the AI space that may not be captured well by just general purpose embedding models because of these two different aspects of this problem fine-tuning can be a good solution starting with that first step of gathering positive and negative pairs to make this a bit more concrete we might have a queer that is something like data pipeline management Advanced SQL techniques ETL elt processes a positive match might be a job description that has something like this make your impact within a rapidly growing fintech company build and manage robust data pipelines that support scalable and then it goes on and on and on basically this is a good match cuz it seems like this role is for a data engineer and this query is also for a data engineer so it seems like this is a good match a negative match on the other hand might look something like this collaborate to design and imp Implement endtoend Ai and data science life cycles from data collection and pre-processing to model deployment and model monitoring so this job on the other hand sounds more like a data scientist or ml engineering role which isn't so well suited for this query so basically what we want to do is gather a bunch of these examples so we can train an embedding model to be more aligned with the behavior that we want it to have while this might sound simple enough this first step is by far the most important and timeconsuming part of this entire fine-tuning process just because you got to ask okay where am I going to get this data from and how am I going to establish positive matches how I'm going to do the negative matches so on and so forth so it may not be obvious how to generate this data but lucky for us I've already gone through the trouble of doing that and I've uploaded the data set on the hugging face Hub so we can just import it with one line of code so I won't walk through all of the code I use to do this because it's quite long and we would spend most of the video talking about it but I will kind of walk walk through the process at a high level and then call out which notebooks and scripts you can check out that correspond to each of these steps and actually this is a more streamlined process than the original version because originally I had gone through a process of scraping a job board to get like a thousand different AI jobs and then go through this whole process but then after I had already uploaded the data and trained the model and all that I realized that the websites terms of use said you can't train AIS on their data so I basically had to start over this is actually a more streamlined process because I just extracted the AI job descriptions from a hugging face data set rather than scraping a website on my own and I guess that's just a good lesson of before you go through the trouble of scraping a website and doing a whole project on it check out the terms of use of that website and then they typically have a robots. text file that you can check out that lets you know what pages are okay or not okay to scrap but just walking through this process step by step the first thing I did was extract the AI job descriptions just imported it from this data set on the hugging face hub from data Stacks called LinkedIn job listing so you can also grab this data set if you like it has 120,000 jobs on it the second thing I did to generate the humanik queries I actually use GPT 40 mini to do this so basically I took the job descriptions from this LinkedIn job listing data set I crafted this prompt for gbt 40 mini that basically said something like given this job description generate a concise like query that corresponds to this description then finally I took all those prompts and then I used open ai's batch API to generate queries for all the job descriptions so if you want to see how I did that you can check out this notebook here it's called one generate synthetic queries. II andb and that's available on the GitHub repository linked here now that we have the humanik queries and we have the job descriptions we'll need to create our positive pairs so basically I took the job descriptions next clean them up and this is a key point so I removed parts that were basically irrelevant to the qualifications of the job boilerplate text that is typically in job descriptions like about the company or equal employment opportunities and things like that I then matched up these cleaned job descriptions to the synthetic queries and then I stored them all in a pandas data frame this is what the result looks like and we could actually stop here because there are fine-tuning approaches that only require positive pairs they don't require negative pairs but I thought to just go one step further and to also get the negative pairs so we can see what that looks like to generate the negative pairs I did something inspired by the generative pseudo labeling paper which I'll pop on the screen here where I basically took all the job descriptions I computed the similarities between them the result of that is this Square Matrix consisting of similarity scores between all the job descriptions and then what I did was for each query I matched the least similar job description as the negative pair the negative example while ensuring there were no repetitions if you're curious about how I did that you can check out the create training data. II Notebook available on the GitHub step two now that we have our data is to pick a pre-trained model and so to do this I went to the sentence Transformers Library so esper.net and there they have these various leaderboards ranking different embedding models so if you go to reference 5 Linked In the description you'll see various base models and their performance on 14 different sentence similarity tasks also if you check out reference six they have various embedding models fine tuned on semantic search specifically so matching queries to relevant items and basically what I did to pick a pre-train model is I passed a handful of different options through this triplet evaluator so this is just a handy object in the sentence Transformers library that allows you to take a data set and create this evaluator with it which you can can pass in a model to and it'll compute the accuracy so I tried just like a few different models nothing too rigorous and found that this one here the all distill Roberta V1 had the best accuracy of about 88% and so I went ahead with this one if you're doing a different kind of task this may not be the best model for you so it's worth reviewing the different options in the sentence Transformers library and seeing what works best on your specific data and then I will point out I'm using that data set I created specifically for this example which has a train test and validation split so here I'm using the validation data to pick the pre-trained model as opposed to the training data or the testing data and that's important because validation data is typically what you use to tune hyperparameters now that we have our data and pre-train model selected next we can select a loss function here I used this handy summary table in the sentence Transformers doc so this is really helpful because they break down what loss functions are appropriate for a given type of data so for example if you just have single sentences and a class so this is some kind of like text classification task they have various loss functions that make sense for that if you're just trying to do domain adaptation on single sentences without a particular label you can use these loss functions they've anchor anchor pairs which not entirely sure what differentiates this from anchor positive pairs but maybe someone will tell me in the comments but for our use case of doing search we're going to be interested in these so anchor would be something like our query and then we have a positive match so we see we have several loss functions that make sense here we might have data structured such that we have an anchor and a positive or negative match and then we have a label one or zero if it's negative so this just like a slightly different way to structure the data and then finally we have this triplet which is what I've talked about here so far so instead of having a flag to indicate whether it's positive or negative you can have the query the positive match and the negative match all grouped together in this triplet here I have data structured like this so we have a few different options and I just went ahead and used this multiple negatives ranking loss they had some example code on their documentation using it so I feel like if they're going to use it for example code it's probably pretty good choice but you know might be worth experimenting with other loss functions and seeing how those work out on this specific use case now that we have our data our model in our loss function we can start training the model we first need to define various hyperparameters so here I followed closely the example code in sentence Transformers documentation just making some small tweaks one important note for contrastive learning which is what this type of machine learning is called where you're training on these positive and negative pairs is you want to have big batch sizes so I was just training this on my laptop so I didn't want to go too crazy so I did a batch size of 16 I I feel like I could have gone bigger actually cuz I just got a new laptop that has 64 GB of memory maybe I could have tried 32 or 64 but this seemed to be a decent starting point at least and then there are various training arguments we can set here so we can Define where we want the model to be saved after training the number of epoch I just went with one the train and evaluation batch size which I just went with 16 learning rate 2 to Theus 5 got this warm-up ratio did this batch sampler no duplicate this was just from sentence Transformers example code basically it ensures that there are no duplicates in each batch because they are randomly sampled and apparently the specific loss function that we used here benefits from no duplicates in the batch eval strategy is basically how often we want to evaluate our loss here I'm going to do it every 100 steps going to compute the loss of the validation data set and then print it out every 100 steps the data set here is about a th000 or I guess 800 training examples so they should print out eight different losses during training those are all our arguments so we can just pass all these into a sentence Transformers trainer which takes things like the model the training arguments the data that we want to use the loss function and the evaluator so this is the same thing I used at the beginning to evaluate the different pre-trained models it's that same object so this took about 45 seconds to run on my new Macbook which is in St contrast to my M1 Mac Mini which this would have taken two 2 hours to run but anyway we can jump to step five and evaluate the model this is just a screenshot from The Notebook on GitHub so I created another one of these triple evaluators but now for the testing data set rather than the validation data and then I just print the accuracy on the validation data set and the testing data set so we can see that the validation performance jumped from 88% to 99% and then we see that the testing performance is 100% this might indicate really good results it also might indicate that the data that we're using is somewhat artificial which is one of the reasons why I would have preferred to use the raw real data I scraped from that website but I think this example still gets the point across of like how to do this whole process even if we are grabbing a data set that's readily available then as a bonus you can use the model so I pushed the model to the hugging face Hub so anyone can use it and then if you want to use it for inference this is the syntax so you can import it using sentence Transformers Define a new query then you can encode the query like this to generate embeddings and then you can take whatever job descriptions you like encode them and compute the similarities between the query and all the different job descriptions okay so the main reason I made this video was to actually set up the next video that I want to make which is fine-tuning multimodal embeddings so I put out a poll on YouTube and Linkedin for video ideas and this was the most popular on both platforms so I thought it would be helpful to the first do fine-tuning text embeddings to set us up for this next one here conceptually we're going to do basically the same thing in the multimodal setting but instead of just operating on text Data we're going to operate on multiple types of data modalities and so one situation where this might be helpful is on YouTube where you have text that comes from titles and then you have images that come from thumbnails and let's say you want to do some kind of thumbnail Search tool or title search tool or you have a really good title idea and you're trying to match it to a thumbnail idea multimodal embeddings would be something you could use for that use case but the problem is if I were to pass this title and this thumbnail into a pre-trained multimodal model like clip we might get low similarity because there's no reason for it to think that these are similar however through fine-tuning so specifically fine-tuning clip on all my titles and all my thumbnails we can adapt this model to say that these two things have high similarity so in the future if I have a title idea or a thumbnail idea I could generate embeddings for it and then try to do some kind of search over a thumbnail library or title library to help in the idea generation process so that's planned for next time and this video and the next video are going to be part of this new fine-tuning playlist that I put together I've realized I've made a handful of videos on fine tuning and I see myself making a handful more this year with my new laptop so I wanted to have a central place for all of the fine-tuning content fine tuning is fun for me and also thinking about product development model fine tuning is an easy way to differentiate your product in a world full of GPT rappers because at the end of the day your product uses a fine-tuned model the value that you're providing is in that data set that you use to fine-tune it rather than the off-the-shelf model that literally everyone in the world has access to so if you're interested in fine-tuning this playlist will be a great resource and if you have any suggestions for future content to cover in this series please let me know in the comment section below and as always thank you so much for your time and thanks for watching\n"
     ]
    }
   ],
   "source": [
    "print(df['title'][3])\n",
    "print(df['transcript'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_strings = ['&#39;', '&amp;', 'sha ']\n",
    "special_string_replacements = [\"'\", \"&\", \"Shaw \"]\n",
    "\n",
    "for i in range(len(special_strings)):\n",
    "    df = df.with_columns(df['title'].str.replace(special_strings[i],special_string_replacements[i]).alias('title'))\n",
    "    df = df.with_columns(df['transcript'].str.replace(special_strings[i], special_string_replacements[i]).alias('transcript'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Text Embeddings For Domain-specific Search (w/ Python)\n",
      "embedding models represent text as semantically meaningful vectors although they can be readily used for countless use cases like retrieval or classification general purpose embedding models may have poor performance on domain specific tasks in this video I'll discuss how we can overcome this limitation via fine tuning I'll start with a highlevel overview of key Concepts and then walk through a concrete example of fine-tuning embeddings on AI J posts okay so let's talk about fine-tuning text embeddings this video is going to assume you already have a basic understanding of what text embeddings do but if you need a refresher I gave a beginner friendly introduction in a previous video so you can check that out if you need to so just jumping right into it Tex embeddings have become more popular these days with the widespread usage of so-called retrieval augmented generation or rag for short and the basic idea behind rag is to improve llm based systems by retrieving relevant content whenever passing a prompt to a large language model a basic example of this is we might have a FAQ bot or a question answering AI assistant where we can pass in a query instead of just passing this query directly to a large language model in a rag system there will be this pre-step of context retrieval and then that query and the context are combined into the prompt which is then fit the large language model to generate a response where embeddings come into this is typically in this context retrieval step the way that works is you'll create this thing called a vector database which has kind of become a buzzword more recently the basic idea of this is taking a set of items in a knowledge base this is typically like chunks of PDF documents or something like that and instead of representing them in their original text in a vector database they are represented by semantic basally meaningful vectors in other words text embedding as a pre-step to building out this retrieval process you'll create this Vector database so at inference time when a query comes in you can simply embed it and then do this so-called similarity search or semantic search where you compute the similarity between the vector representation of the query and all the different items in your knowledge base or your vector database although doing retrieval in this way has become very popular in rag system systems there's one key problem with it and it boils down to this similar doesn't necessarily mean helpful so let me break this down a little bit here in similarity search or also called semantic search we have a vector representation of a query and we have Vector representations of items in a knowledge base we can compute search results by Computing the similarity between this Vector representation of the query and all the items in the knowledge base to make this more concrete the query might be something like how do I update my payment method this might be a very common question so we would expect that it's pretty easy for our system to answer something like this however in practice one thing that might happen is when we do this similarity search the results might be something like this where the top result so in other words the most similar chunk is something like to view your payment history visit the billing section of your account the second most similar result might be payments can be made via credit cards debit cards a and mobile payments it's only the third most similar result that we get something that seems helpful which is to update billing details visit the billing section of your account so this is kind of like the central issue with similarity search which is that just because a query and an item in your knowledge base are similar that doesn't necessarily mean that the item is helpful in answering the query so one thing we can do to overcome these issues is to fine-tune and embedding model for a specific use case so the basic idea of fine-tuning is to adapt a model to a particular use case through additional training so the intuition here is that we might have an original base model which is like a blank slab of marble and the fine tuning process is basically just refining and chiseling this slab into something that is more appropriate for our use case and then of course there's nothing special about a fine-tuned model and it's common practice to fine-tune models multiple times to give it different behaviors and so we we could do additional fine tuning to take a fine-tuned model and make it even more optimized for our use case and so I've actually made a handful of videos on fine-tuning if this is a new idea to you I would recommend checking those out but specifically when it comes to text embeddings the central motivation for fine-tuning is that this idea of similarity varies by use case for example consider these two pieces of text we have one that says what is Rag and another one that says what is fine tun so let's say we were trying to train a comment classifier to basically take YouTube comments and cluster similar ones together and ensure that different ones aren't clustered together in that case these two questions should be represented in similar Ways by our embedding model so that when we do the clustering they will be put into the same group however if we were doing something like an FAQ search the questions what is Rag and what is fine tuning these should be dissimilar things because if we have an incoming query that says what is rag an item in the knowledge base that contains the text what is fine-tuning wouldn't be helpful in answering this query so ideally these should be dissimilar and taking this one step further if we were talking about a different context so instead of doing FAQ search in the context of AI let's say we're doing document search in the context of project management where rag instead of standing for retrieval augmented generation it instead stands for red Amber and green which is like this stoplight system used sometimes in project management then these items should again be dissimilar because this question isn't helpful in answering this query but the representations should be different because the meaning of rag in the context of project management is much different than its meaning in AI so here I'll break down the embedding fine-tuning process into five basic steps the first step is to get rather positive and negative pairs the second step is to pick a pre-trained model and we'll see how we can do this the third step is to pick a loss function which will basically guide the fine-tuning process and then this will depend on the task that you're trying to do and the data that you have available the fourth step is you're going to fine-tune the pre-trained model using your data set and then finally we're going to evaluate the model so instead of talking about these five steps in an abstract way I think it would be helpful to walk through a concrete example so we can see what each of these steps looks like in code so here I'm going to fine-tune a set of text embeddings on AI job listings the basic idea is here we want to take humanlike queries like data scientist 6 years of experience llms credit risk content marketing and we want to match this query with some kind of job post there are two things here that make fine-tuning helpful for this task one is that the the queries and the job descriptions look very different queries are typically very short while job descriptions can be quite long there's a need right there for some kind of fine tuning and additionally there's a lot of technical jargon in the AI space that may not be captured well by just general purpose embedding models because of these two different aspects of this problem fine-tuning can be a good solution starting with that first step of gathering positive and negative pairs to make this a bit more concrete we might have a queer that is something like data pipeline management Advanced SQL techniques ETL elt processes a positive match might be a job description that has something like this make your impact within a rapidly growing fintech company build and manage robust data pipelines that support scalable and then it goes on and on and on basically this is a good match cuz it seems like this role is for a data engineer and this query is also for a data engineer so it seems like this is a good match a negative match on the other hand might look something like this collaborate to design and imp Implement endtoend Ai and data science life cycles from data collection and pre-processing to model deployment and model monitoring so this job on the other hand sounds more like a data scientist or ml engineering role which isn't so well suited for this query so basically what we want to do is gather a bunch of these examples so we can train an embedding model to be more aligned with the behavior that we want it to have while this might sound simple enough this first step is by far the most important and timeconsuming part of this entire fine-tuning process just because you got to ask okay where am I going to get this data from and how am I going to establish positive matches how I'm going to do the negative matches so on and so forth so it may not be obvious how to generate this data but lucky for us I've already gone through the trouble of doing that and I've uploaded the data set on the hugging face Hub so we can just import it with one line of code so I won't walk through all of the code I use to do this because it's quite long and we would spend most of the video talking about it but I will kind of walk walk through the process at a high level and then call out which notebooks and scripts you can check out that correspond to each of these steps and actually this is a more streamlined process than the original version because originally I had gone through a process of scraping a job board to get like a thousand different AI jobs and then go through this whole process but then after I had already uploaded the data and trained the model and all that I realized that the websites terms of use said you can't train AIS on their data so I basically had to start over this is actually a more streamlined process because I just extracted the AI job descriptions from a hugging face data set rather than scraping a website on my own and I guess that's just a good lesson of before you go through the trouble of scraping a website and doing a whole project on it check out the terms of use of that website and then they typically have a robots. text file that you can check out that lets you know what pages are okay or not okay to scrap but just walking through this process step by step the first thing I did was extract the AI job descriptions just imported it from this data set on the hugging face hub from data Stacks called LinkedIn job listing so you can also grab this data set if you like it has 120,000 jobs on it the second thing I did to generate the humanik queries I actually use GPT 40 mini to do this so basically I took the job descriptions from this LinkedIn job listing data set I crafted this prompt for gbt 40 mini that basically said something like given this job description generate a concise like query that corresponds to this description then finally I took all those prompts and then I used open ai's batch API to generate queries for all the job descriptions so if you want to see how I did that you can check out this notebook here it's called one generate synthetic queries. II andb and that's available on the GitHub repository linked here now that we have the humanik queries and we have the job descriptions we'll need to create our positive pairs so basically I took the job descriptions next clean them up and this is a key point so I removed parts that were basically irrelevant to the qualifications of the job boilerplate text that is typically in job descriptions like about the company or equal employment opportunities and things like that I then matched up these cleaned job descriptions to the synthetic queries and then I stored them all in a pandas data frame this is what the result looks like and we could actually stop here because there are fine-tuning approaches that only require positive pairs they don't require negative pairs but I thought to just go one step further and to also get the negative pairs so we can see what that looks like to generate the negative pairs I did something inspired by the generative pseudo labeling paper which I'll pop on the screen here where I basically took all the job descriptions I computed the similarities between them the result of that is this Square Matrix consisting of similarity scores between all the job descriptions and then what I did was for each query I matched the least similar job description as the negative pair the negative example while ensuring there were no repetitions if you're curious about how I did that you can check out the create training data. II Notebook available on the GitHub step two now that we have our data is to pick a pre-trained model and so to do this I went to the sentence Transformers Library so esper.net and there they have these various leaderboards ranking different embedding models so if you go to reference 5 Linked In the description you'll see various base models and their performance on 14 different sentence similarity tasks also if you check out reference six they have various embedding models fine tuned on semantic search specifically so matching queries to relevant items and basically what I did to pick a pre-train model is I passed a handful of different options through this triplet evaluator so this is just a handy object in the sentence Transformers library that allows you to take a data set and create this evaluator with it which you can can pass in a model to and it'll compute the accuracy so I tried just like a few different models nothing too rigorous and found that this one here the all distill Roberta V1 had the best accuracy of about 88% and so I went ahead with this one if you're doing a different kind of task this may not be the best model for you so it's worth reviewing the different options in the sentence Transformers library and seeing what works best on your specific data and then I will point out I'm using that data set I created specifically for this example which has a train test and validation split so here I'm using the validation data to pick the pre-trained model as opposed to the training data or the testing data and that's important because validation data is typically what you use to tune hyperparameters now that we have our data and pre-train model selected next we can select a loss function here I used this handy summary table in the sentence Transformers doc so this is really helpful because they break down what loss functions are appropriate for a given type of data so for example if you just have single sentences and a class so this is some kind of like text classification task they have various loss functions that make sense for that if you're just trying to do domain adaptation on single sentences without a particular label you can use these loss functions they've anchor anchor pairs which not entirely sure what differentiates this from anchor positive pairs but maybe someone will tell me in the comments but for our use case of doing search we're going to be interested in these so anchor would be something like our query and then we have a positive match so we see we have several loss functions that make sense here we might have data structured such that we have an anchor and a positive or negative match and then we have a label one or zero if it's negative so this just like a slightly different way to structure the data and then finally we have this triplet which is what I've talked about here so far so instead of having a flag to indicate whether it's positive or negative you can have the query the positive match and the negative match all grouped together in this triplet here I have data structured like this so we have a few different options and I just went ahead and used this multiple negatives ranking loss they had some example code on their documentation using it so I feel like if they're going to use it for example code it's probably pretty good choice but you know might be worth experimenting with other loss functions and seeing how those work out on this specific use case now that we have our data our model in our loss function we can start training the model we first need to define various hyperparameters so here I followed closely the example code in sentence Transformers documentation just making some small tweaks one important note for contrastive learning which is what this type of machine learning is called where you're training on these positive and negative pairs is you want to have big batch sizes so I was just training this on my laptop so I didn't want to go too crazy so I did a batch size of 16 I I feel like I could have gone bigger actually cuz I just got a new laptop that has 64 GB of memory maybe I could have tried 32 or 64 but this seemed to be a decent starting point at least and then there are various training arguments we can set here so we can Define where we want the model to be saved after training the number of epoch I just went with one the train and evaluation batch size which I just went with 16 learning rate 2 to Theus 5 got this warm-up ratio did this batch sampler no duplicate this was just from sentence Transformers example code basically it ensures that there are no duplicates in each batch because they are randomly sampled and apparently the specific loss function that we used here benefits from no duplicates in the batch eval strategy is basically how often we want to evaluate our loss here I'm going to do it every 100 steps going to compute the loss of the validation data set and then print it out every 100 steps the data set here is about a th000 or I guess 800 training examples so they should print out eight different losses during training those are all our arguments so we can just pass all these into a sentence Transformers trainer which takes things like the model the training arguments the data that we want to use the loss function and the evaluator so this is the same thing I used at the beginning to evaluate the different pre-trained models it's that same object so this took about 45 seconds to run on my new Macbook which is in St contrast to my M1 Mac Mini which this would have taken two 2 hours to run but anyway we can jump to step five and evaluate the model this is just a screenshot from The Notebook on GitHub so I created another one of these triple evaluators but now for the testing data set rather than the validation data and then I just print the accuracy on the validation data set and the testing data set so we can see that the validation performance jumped from 88% to 99% and then we see that the testing performance is 100% this might indicate really good results it also might indicate that the data that we're using is somewhat artificial which is one of the reasons why I would have preferred to use the raw real data I scraped from that website but I think this example still gets the point across of like how to do this whole process even if we are grabbing a data set that's readily available then as a bonus you can use the model so I pushed the model to the hugging face Hub so anyone can use it and then if you want to use it for inference this is the syntax so you can import it using sentence Transformers Define a new query then you can encode the query like this to generate embeddings and then you can take whatever job descriptions you like encode them and compute the similarities between the query and all the different job descriptions okay so the main reason I made this video was to actually set up the next video that I want to make which is fine-tuning multimodal embeddings so I put out a poll on YouTube and Linkedin for video ideas and this was the most popular on both platforms so I thought it would be helpful to the first do fine-tuning text embeddings to set us up for this next one here conceptually we're going to do basically the same thing in the multimodal setting but instead of just operating on text Data we're going to operate on multiple types of data modalities and so one situation where this might be helpful is on YouTube where you have text that comes from titles and then you have images that come from thumbnails and let's say you want to do some kind of thumbnail Search tool or title search tool or you have a really good title idea and you're trying to match it to a thumbnail idea multimodal embeddings would be something you could use for that use case but the problem is if I were to pass this title and this thumbnail into a pre-trained multimodal model like clip we might get low similarity because there's no reason for it to think that these are similar however through fine-tuning so specifically fine-tuning clip on all my titles and all my thumbnails we can adapt this model to say that these two things have high similarity so in the future if I have a title idea or a thumbnail idea I could generate embeddings for it and then try to do some kind of search over a thumbnail library or title library to help in the idea generation process so that's planned for next time and this video and the next video are going to be part of this new fine-tuning playlist that I put together I've realized I've made a handful of videos on fine tuning and I see myself making a handful more this year with my new laptop so I wanted to have a central place for all of the fine-tuning content fine tuning is fun for me and also thinking about product development model fine tuning is an easy way to differentiate your product in a world full of GPT rappers because at the end of the day your product uses a fine-tuned model the value that you're providing is in that data set that you use to fine-tune it rather than the off-the-shelf model that literally everyone in the world has access to so if you're interested in fine-tuning this playlist will be a great resource and if you have any suggestions for future content to cover in this series please let me know in the comment section below and as always thank you so much for your time and thanks for watching\n"
     ]
    }
   ],
   "source": [
    "print(df['title'][3])\n",
    "print(df['transcript'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>video_id</th><th>datetime</th><th>title</th><th>transcript</th></tr><tr><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;bZr2vhoXSy8&quot;</td><td>2025-02-08 18:10:05</td><td>&quot;I Trained FLUX.1 on My Face (P…</td><td>&quot;flux is a state-of-the-art ima…</td></tr><tr><td>&quot;QvxuR8uLPFs&quot;</td><td>2025-02-03 18:00:00</td><td>&quot;How to Build Customer Segments…</td><td>&quot;although today&#x27;s AI models are…</td></tr><tr><td>&quot;W4s6b2ZM6kI&quot;</td><td>2025-01-31 22:38:22</td><td>&quot;Fine-tuning Multimodal Embeddi…</td><td>&quot;multimodal embedding models br…</td></tr><tr><td>&quot;hOLBrIjRAj4&quot;</td><td>2025-01-22 21:25:16</td><td>&quot;Fine-Tuning Text Embeddings Fo…</td><td>&quot;embedding models represent tex…</td></tr><tr><td>&quot;V1BR2tb_e8g&quot;</td><td>2025-01-13 21:10:47</td><td>&quot;My AI Development Setup (From …</td><td>&quot;hey everyone I&#x27;m Shaw I just g…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────┬─────────────────────┬───────────────────────────────┬──────────────────────────────┐\n",
       "│ video_id    ┆ datetime            ┆ title                         ┆ transcript                   │\n",
       "│ ---         ┆ ---                 ┆ ---                           ┆ ---                          │\n",
       "│ str         ┆ datetime[μs]        ┆ str                           ┆ str                          │\n",
       "╞═════════════╪═════════════════════╪═══════════════════════════════╪══════════════════════════════╡\n",
       "│ bZr2vhoXSy8 ┆ 2025-02-08 18:10:05 ┆ I Trained FLUX.1 on My Face   ┆ flux is a state-of-the-art   │\n",
       "│             ┆                     ┆ (P…                           ┆ ima…                         │\n",
       "│ QvxuR8uLPFs ┆ 2025-02-03 18:00:00 ┆ How to Build Customer         ┆ although today's AI models   │\n",
       "│             ┆                     ┆ Segments…                     ┆ are…                         │\n",
       "│ W4s6b2ZM6kI ┆ 2025-01-31 22:38:22 ┆ Fine-tuning Multimodal        ┆ multimodal embedding models  │\n",
       "│             ┆                     ┆ Embeddi…                      ┆ br…                          │\n",
       "│ hOLBrIjRAj4 ┆ 2025-01-22 21:25:16 ┆ Fine-Tuning Text Embeddings   ┆ embedding models represent   │\n",
       "│             ┆                     ┆ Fo…                           ┆ tex…                         │\n",
       "│ V1BR2tb_e8g ┆ 2025-01-13 21:10:47 ┆ My AI Development Setup (From ┆ hey everyone I'm Shaw I just │\n",
       "│             ┆                     ┆ …                             ┆ g…                           │\n",
       "└─────────────┴─────────────────────┴───────────────────────────────┴──────────────────────────────┘"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.write_parquet('data/video-transcripts.parquet)')\n",
    "df.write_csv('data/video-transcripts.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
